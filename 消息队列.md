# 比较

|                         | ActiveMQ         | RabbitMQ         | RocketMQ                              | Kafka                                                |
| ----------------------- | ---------------- | ---------------- | ------------------------------------- | ---------------------------------------------------- |
| 功能                    | MQ功能极其完备   | MQ功能完备       | MQ功能较为完善                        | MQ功能较为简单，主要使用在大数据的实时计算和日志采集 |
| 单机吞吐量              | 万级             | 万级             | 十万级                                | 十万级                                               |
| 时效性                  | ms级             | μs级（延迟最低） | ms级                                  | ms级以内                                             |
| 可用性                  | 高，基于主从架构 | 高，基于主从架构 | 非常高，分布式架构                    | 非常高，分布式架构，一个数据多个副本                 |
| 消息可靠性              |                  |                  |                                       | 会丢数据                                             |
| topic数量对吞吐量的影响 |                  |                  | topic达到百级时，吞吐量会有小幅度下降 | topic达到千级时，吞吐量会大幅下降                    |



# Q：消息队列如何保证消息顺序性？

## rabbitMQ

每个消费者读取一个queue

## kafka

每个消费者**只能消费一个**partition，若消费者比partition多，则有消费者拿不到数据，没有意义

写入同一个partition的数据一定是有顺序的，因此生产者写入时，可以指定一个key，则相同key一定会分发到同一个partition

![image-20220318015558871](images\消息队列\image-20220318015558871.png)





高可用

![image-20220420105234912](images/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/image-20220420105234912.png)

# Q：如何保证消息不丢失（数据一致性）？

## Kafka

**情形1：消费者丢数据**

kafka消费者会自动提交offset（记为i），broker就认为i号消息已被消费，若此时消费者出问题，则下次请求broker给的就是i+1号消息了，i号消息丢失。

解决：改为手动提交

```
request.required.asks=0
# 0:相当于异步的, 不需要leader给予回复, producer立即返回, 发送就是成功,
那么发送消息网络超时或broker crash(1.Partition的Leader还没有commit消息 2.Leader与
Follower数据不同步), 既有可能丢失也可能会重发
# 1：当leader接收到消息之后发送ack, 丢会重发, 丢的概率很小
# -1：当所有的follower都同步消息成功后发送ack. 不会丢失消息
```

**情形2：broker丢数据**

某个broker宕机，重新选举leader，此时follower数据未同步完，leader也挂了，则leader上的数据丢失。

解决：给这个topic设置`replication.factor`大于1，即每个partition至少有2个副本

broker设置`min.insync.replicas`大于1，要求leader至少有一个follower与自己保持同步中

生产者设置`acks = all`，要求每条数据必须写入所有replica后，才认为写成功

生产者设置`retries = MAX（最大整数）`，要求写入失败时，无限重试

**Q：生产者是否会丢数据？**

按照情形2的解决办法，能保证生产者一定将数据发送到broker，且存有2个以上副本，若不满足，生产者会不断重试

## RocketMQ



# Q：如何保证消息不重复（幂等性）？

kafka消费者会在获取消息后提交offset（相当于序号）给zk，告诉broker消费到哪了，从而保证消费者获取消息的基本顺序性，当消费者宕机重启后，还能从上次消费的地方继续开始

然而消费者并不是每条消息都提交offset，而是定时批量提交。若消费到中间某个消息（序号i），还未提交offset就宕机，则下次重启时无法继续从消息i开始，而是从i之前的某条开始，这就会出现重复消息

**解决**

1. 用redis记录消息id（流水号），重复就跳过
2. 用数据库唯一主键约束
3. 加分布式锁（实际项目），一条消息从生产到消费全程加锁



# Kafka

## Broker 

Server. 包含多个 Topic , Partition, 和 Replica. 负责协调 Producer 和 Consumer 主从结构为: 主节点为 Controller, 从节点为从节点 Kafka 启动是会往 Zookeeper 中注册当前 Broker 信息. 谁先注册谁就是 Controller. 读取注册上来的从节点的数据(通过监听机制), 生成集群 的元数据信息, 之后把这些信息都分发给其他的服务器, 让其他服务器能感知到集群中其它成员的 存在

## Topic 

标准 MQ 中的 Queue. Kafka 中一个 Topic 的消息会保存在不同的 Partition (不同的 Broker)来保 证高可用

## Partition (分区) 

可以理解为将标准 MQ 的 Queue 的消息进行拆分, 来实现高可用 Producer 发送的 Message, 根据 key 和 partition 数进行 hash, 然后进行投递 一个分区只能被同一个 Consumer Group 中的一个 Consumer 消费. 分区内消费有序

## Replica (备份)

 每一个 Partition 的备份. Replica 的小于等于 Broker 的数量 Leader: Replica领导节点, 每一个 Partition 都有对应的 Leader 节点(Broker). Producer 写数据 时, 只会往 Leader 中写. Consumer 读数据也是从 Leader 中读 Follower: Replica跟随节点, 用于 复制领导节点的数据. 复制 Leader 消息采用 pull (拉)模式

## Producer

 标准 MQ 中的发送方. 发送给 Broker 使用push (推)模式

# CTG-MQ

基于RocketMQ，提供高可用

## Name Server

一个几乎无状态节点，可集群部署，节点之间无同步信息。 它主要提供broker注册、Topic路由管理等功能

## Broker

消息中转角色，负责存储消息，转发消息，一般也称Server。

CTG-MQ一般在多个服务器部署broker集群

## Topic 

在ctg-mq中，topic类似于JMS规范中的队列，所有消息都是存放在不同的 topic中，生产都与消费者都以topic名字进行生产与消费。

broker与topic是多对多关系，broker里可能有多个topic，一个Topic可以分片存在多个broker上（分布式）。

每个Topic由多个队列组成（类似kafka的分区）。



![image-20220426204242838](images/消息队列/image-20220426204242838.png)

## 生产者

 生产组：一类Producer的集合名称，这类Producer通常发送一类消息（同一个topic），且发送逻辑一致，一般由业务系统负责产生消息。

生产者实例： 一个生产者实例代表**生产者的一员**，不同的生产者用不同的实例名字创建。

## 消费者

消费组：一类Consumer的集合名称，这类Consumer通常消费一类消息（同一个topic），且消费逻辑一致，一般是后台系统负责异步消费。消费进度由存储在消费组上。

消费者实例 ：一个消费者实例代表**消费组的一员**，不同的消费者用不同的实例名字创建。

**消费模型**：主要有push和pull两种

### PUSH模型

推送模型（被动），是一个“发布-订阅”模型，由consumer封装了轮询过程，并注册MessageListener监听器，consumer接收消息后，回调监听器接口，唤醒监听器来消费，对用户而言，感觉消息是被推送过来的。

优点是实时性好，缺点是需要维护一个长连接，资源消耗大。该模型适合于的场景：client数量不多，server数据变化频繁。

在ctg-mq中，客户端会自动开启线程消费消息，线程数是5-64，即listener里面的方法，会被多线程执行。

客户端内部可以根据堆积量进行调整，使用者不需要新启、管理消费线程。并有流控机制，当客户端缓存一定量消息，导致消费不及时，会停止推送新消息。

### PULL模型

拉取模型（主动），应用通常主动调用Consumer的拉消息方法从Broker拉 消息，主动权由应用控制，但实时性取决于应用主动拉取的频率。在PULL消费中，线程由应用自主决定。

consumer在pull的时候，告诉broker自己buffer中可用的容量，整个流程如下：

1、consumer请求broker，告诉broker本地的可承载量，比如500
2、broker在收到消息后，如果没有消息则进入long polling状态
3、当有消息的时候，broker直接向consumer进行push，总共push的数据量为500
4、在整个push期间，consumer无需重新pull，即可获取数据
5、由于broker知道最大容量，所以无需担心被冲垮。

广播消费 

注意：使用消费模式，在很多使用场景都会带来影响或限制，在CTG-MQ中， 应尽量避免使用此消费模式。 在ctg-mq中，消费者有两种不同的方式消费topic中的消息，其中一种是广 播消费。在广播消费模式下，一条消息被多个Consumer消费，即使这些 Consumer属于同一个Consumer Group，消息也会被Consumer Group中 的每个Consumer都消费一次，广播消费中的Consumer Group概念可以认 为在消息划分方面无意义。 V1.x版本由于广播消费的消费进度，是保存在客户端的，对于很多使用场景 会带来影响，在ctg-mq中，并不推荐使用此消费模式。 V2.x版本在服务端存储消费状态，不支持此消费模式。推荐通过多消费组 （订阅组）的方式进行消费。

集群消费 

一 个 Topic 可 以 被 一 个 或 多 个 Consumer Group 消 费 ， 每 个 Consumer Group有自己独立的消费进度，消费进度是保存在服务端的。 一个Consumer Group中的消费者实例可以平均分摊消费消息，做到负载均 衡。例如某个Topic有9条消息，其中一个Consumer Group有3个不同的消 费者实例（可能是3个进程，或者3台机器），那么每个实例只消费其中的3 条消息。 在此消费模式下，可以做到Point-To-Point的消费，也可以做到JMS里面广 播消费，能满足绝大部分场景，推荐使用此消费模式。



使用开源RocketMQ遇到的问题

  功能不完善：功能比较单一，针对不同应用场景无法有效支 持，如消息轨迹查询，严格消费机制，数据自动删除策略等。

  可维护性差：缺乏配套监控运维能力，难以迅速发现解决如 消息堆积、队列堵塞等问题。  可靠性较低：消息服务不提供主备切换能力，存在单点故障， 无法保证服务高可用

改进点：

  高可用、高可靠改进： 

 实现自动主备切换、自动拉起功能，保证服务高可用 

 实现消息删除策略，按不同的场景优先保证服务可用性或者 数据安全性。

  可维护性改进：  实现按生产者、消费者、数据节点、队列4种维度的运行状态 监控，方便快速发现问题

  实现可视化的监控、配置、管理界面 

 实现自动化测试，以快速迭代 

 新增功能： 

 新增消息轨迹收集、分析与查询，做到可查可追踪

  新增严格的消费机制，满足消费严格不重复的应用需求 

 重新封装SDK，简化应用使用，并提供按hash算法实现消息 局部有序生产消费

## 分布式

## 消息模式

**有序消息**

消费消息的顺序要同发送消息的顺序一致，在CTG-MQ中，主要有两种有序 消息

**普通有序消息** 

在正常情况下可以保证完全的顺序消息，但是一旦发生通 信异常，Broker重启，由于队列总数发生变化，哈希取模后定位的队列会变 化，产生短暂的消息顺序不一致。 如果业务能容忍在集群异常情况（如某个Broker宕机或者重启）下，消息短 暂的乱序，使用普通顺序方式比较合适。

**严格有序消息** 

无论正常异常情况都能保证顺序，但是牺牲了分布式 Failover特性，即Broker集群中只要有一台机器不可用，则整个集群都不可 用（或者影响hash值对应队列的使用），服务可用性大大降低。 如果服务器部署为同步双写模式，此缺陷可通过备机自动切换为主避免，不 过仍然会存在几分钟的服务不可用。 若业务能容忍短暂乱序，推荐普通有序消费。

在CTG-MQ中，消息模式选择的优先顺序为：**无序消息>普通有序消息>严格有序消息**。在业务场景允许的情况下，优先选择无序消息，或者 在业务能变通的情况下，将有序消息转化为无序消息。

无序消息的优点：

1. 生产者可以使用多进程、多线程往同一个TOPIC发送，性能较好
2. 消费者可以使用多进程、多线程同时消费，性能较好 
3. 可以充分使用集群的failover（故障转移）特点，无须依赖自动主备切换（切换过期服务中断），包括：  当集群中某一broker节点故障时，不影响业务消息生产， 消息将failover发送到其它节点  当集群中某一broker节点故障时，不影响其它节点数据 消费，故障恢复后既可消费  * 充分利用failover特点，可不部署自动切换组件，减少 部署复杂度，减少运维难度
4. 能动态地扩容



 有序消息的缺点 + 必须注意事项：

1. 对于生产者来说，因为要保证顺序，所以一般都是单线程串行发送， 性能较低。对一个Topic有多个Queue的场景，可以通过程序控制 一个生产者发送固定的Queue，增加生产者数，提高性能。 

2. 对于消费者来说，因为要保证顺序，所以一般是单线程串行消费，消 费性能较低。对一个Topic有多个Queue的场景，可以通过增加消费者 实例数，提高性能，但应用必须考虑消息者实例数量变化，由于负载均 衡带来的短时间数据乱序问题。

3. 对于普通有序消息，当节点故障时，由于Queue数的变化，导致hash 值的变化，产生与消费都会出现短暂的消息顺序不一致；对于严格有序 消息，当节点故障时，Queue数不会变化，产生与消费都会出现异常， 直到故障节点恢复。

4. 对于普通有序消息，意味着业务能接受短时间消息乱序，所以一般情 况下可以在线态扩容；对于严格有序消息，需要将所有消息消费完，并 且停止服务，才能扩容。

5. 有一种场景，同一Topic所有消息必须是有序生产与有序消费，可以使 用多Queue然后在生产消费端做顺序处理，也可以拆分多个Topic然后 使用单Queue进行处理，但在设计时，应用必须考虑单Queue的处理性 能是否能满足。对于单Queue，只要对应的broker故障，则服务中断。
